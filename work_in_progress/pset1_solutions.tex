\documentclass[11pt]{extarticle}
\usepackage{fullpage,amsmath,amsfonts,microtype,nicefrac,amssymb, amsthm}
\usepackage[left=1in, bottom=1in, top=1in, right = 1in]{geometry}
\usepackage{textcomp}
\usepackage{mathpazo}
\usepackage{mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}

\usepackage{microtype}

\usepackage{bm}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{ragged2e}

\setlength{\parindent}{24pt}
\setlength{\jot}{8pt}


\usepackage[shortlabels]{enumitem}


%% FOOTNOTES
\usepackage[bottom]{footmisc}
\usepackage{footnotebackref}


%% FIGURE ENVIRONMENT
%\graphicspath{{}}
\usepackage[margin=15pt, font=small, labelfont={bf}, labelsep=period]{caption}
\usepackage{subcaption}
\captionsetup[figure]{name={Figure}, position=above}
\usepackage{float}
\usepackage{epstopdf}


%% NEW COMMANDS
\renewcommand{\baselinestretch}{1.25} 
\renewcommand{\qedsymbol}{$\blacksquare$}
\newcommand{\R}{\mathbb{R}}
\newcommand{\indep}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}
\renewcommand{\b}{\begin}
\newcommand{\e}{\end}

%% NEWTHEOREM
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}
\newtheorem{remark}[thm]{Remark}
\newtheorem{cor}[thm]{Corollary}

%% LINKS and COLORS
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\definecolor{myred}{RGB}{163, 32, 45}
\hypersetup{
	%backref=true,
	%pagebackref=true,
	colorlinks=true,
	urlcolor=myred,
	citecolor=myred, 
	linktoc=all,     
	linkcolor=myred,
}

%% TABLE OF CONTENTS
\addto\captionsenglish{
	\renewcommand{\contentsname}
	{}% This removes the heading over the table of contents.
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%            END PREAMBLE           %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Dynamic Optimization: Problem Set \#1}

\author{Andreas Schaab}

\date{Fall, 2022}



\begin{document}

\maketitle
\thispagestyle{empty}
\setcounter{page}{0}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{10mm}
\section*{Problem 1}

\textbf{Credit:} QuantEcon \url{https://python.quantecon.org/finite_markov.html#exercises}
 
\vspace{5mm}
\noindent
Let $y_t$ denote the employment / earnings state of an individual. Consider the state space $y_t \in Y = \{ y^U, y^E \}$, where $y^U$ corresponds to unemployment and $y^E$ corresponds to employment. Let $\bm y$ denote the column vector $(y^U, y^E)'$ representing this state space (this is the grid you would construct on a computer). Suppose the employment dynamics of the individual are characterized by the invariant transition matrix 
\begin{equation*}
	P = \begin{pmatrix} \alpha & 1 - \alpha \\ 1 - \beta & \beta \end{pmatrix}.
\end{equation*}
We interpret a time period as a quarter. 

\begin{enumerate}[(a)]
\item Give economic interpretations of $\alpha = P_{11}$ and $\beta = P_{22}$\\
\textcolor{blue}{$\alpha = P_{11}$: Probability that an unemployed doesn't find a job and remains unemployed at t.
$\beta = P_{22}$: Probability that an employed doesn't lose the job and remains employed at t. }

\item Why do the rows of $P$ sum to $1$?\\
\textcolor{blue}{Because each row is a probability distribution. Intuitively: in the model either you are employed or unemployed}

\item Is there an absorbing state in this model?\\
\textcolor{blue}{We say $x$ is an absorbing state if $$P(X_{t+1}=x|X_t=x)=1$$ There are no absorbing states in this model if $\beta$,$\alpha \in (0,1)$ .}

\item Compute the probability of being unemployed two quarters after being employed. \\
\textcolor{blue}{$$P(y_{t+2}=y^U,y_{t+1}=y^U|y_t=y^E)+P(y_{t+2}=y^U,y_{t+1}=y^E|y_t=y^E)=$$ $$\alpha (1-\beta)+(1-\beta)\beta = (\alpha+\beta)(1-\beta)$$}

\item Denote the \textit{marginal (probability) distribution} of $y_t$ at time $t$ by $\psi_t$. $\psi_t(y^L)$ is the probability that process $y_t$ is in state $y^L$ at time $t$. It is easiest to think of $\psi_t$ as a time-varying row vector. Use the law of total probability to decompose $y_{t+1} = y^L$, accounting for all the possible ways in which state $y^L$ can be reached at time $t+1$. \\
\textcolor{blue}{$$\psi_{t+1}(y^L) =  P(y_{t+1}=y^L)=\psi_t(y^L)P(y_t+1=y^L|y_t=y^L)+(1-\psi_t(y^L))P(y_{t+1}=y^L|y_t\neq y^L)$$ }

\item Show that the resulting equation can be written as the vector-matrix product
\begin{equation*}
	\psi_{t+1} = \psi_t P.
\end{equation*}
Therefore: The evolution of the marginal distribution of a Markov chain is obtained by post-multiplying by the transition matrix. 

\textcolor{blue}{It is clear that the conditional probabilities form question e) are obtained from one column of the transition matrix P, so we can write it as the vector-matrix product. }
\item Show that
\begin{equation*}
	X_0 \sim \psi_0 \implies X_t \sim \psi_0 P^t,
\end{equation*}
where $\sim$ reads ``is distributed according to''. \\
\textcolor{blue}{First $\psi_1=\psi_0 P$,  then also $\psi_2 = \psi_1 P = \psi_0 P^2 $. Iterating we get the result }

\item We call $\psi^*$ a \textit{stationary distribution} of the Markov chain if it satisfies 
\begin{equation*}
	\psi^* = \psi^* P.
\end{equation*}
Compute the probability of being unemployed $n$ quarters after being employed. Take $n \to \infty$ and find the stationary distribution of this Markov chain. Find the stationary distribution by alternatively plugging into the above equation for $\psi^*$. \\
\textcolor{blue}{As a summary (Check the link form quantecon for more detail): A Markov Chain is irreducible if all states communicate, that is there is a positive probability of going from any y to any x (possibly in many steps). Then an irreducible MC converges to a stationary (or ergodic) distribution. So if we take $n\rightarrow \infty$ the marginal distribution converges to the stationary: $\psi_{t+n}\rightarrow \psi^*$.\\
To find the stationary distribution we need to solve the equation above. Note $\psi =0$ is a solution but it's not a probability distribution. Let $\psi^* = (\psi^*(y^U),1-\psi^*(y^U))$, then solving the equation we get $$\psi^*(y^U)=\frac{1-\beta}{2-\alpha - \beta }$$
Economic intuition: Spend more time unemployed (or unemployment rate higher) if the probability of finding a job is lower (the job finding rate) or the probability of losing job is higher (the separation rate). }
\item Suppose $y_0 = y^H$. Solve for $\mathbb E_0 (y_t)$. Use the law of total / iterated expectation to relate expectation to probabilities. Then use the formulas for marginal (probability) distributions derived above. 
\textcolor{blue}{$$\mathbb E_0 (y_t)=P(y_t=y^U|y_0=y^E)y^U+P(y_t=y^E|y_0=y^E)y^E $$ And we get the transition probabilities from $P^t$ }
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{10mm}
\section*{Problem 2}

Consider the first-order linear \textit{homogeneous} difference equation 
\begin{equation*}
	x_{t+1} = \rho x_t.
\end{equation*}
We parameterize the initial condition by $x_0 = c$. 

\begin{enumerate}[(a)]
\item Prove by induction that the \textit{general solution} (for arbitrary $c$) is given by
\begin{equation*}
	x_t = \rho^t c.
\end{equation*}
\textcolor{blue}{Guess $x_t=\rho^t c$. Check true at $t= 0$ : $x_0 = \rho^0c=c$. If true at $t$ also true at $t+1$: $$x_{t+1}=\rho x_t = \rho \rho^t c= \rho^{t+1}c$$}

\item Show that the \textit{particular solution} for initial value $x_0 = x$ is given by $x_t = \rho^t x$. 
\textcolor{blue}{If $x_0=x$ then set $c=x$.}

\item Show that this also implies 
\begin{equation*}
	x_t = \rho^{t-s} x_s
\end{equation*}
for $t > s$. \\
\textcolor{blue}{We have $c=\frac{x_s}{\rho^s}$ for any s, substitute in the general solution from (a) and we get the result.}

\item Prove that this difference equation satisfies the Markov property. \\
\textcolor{blue}{Note we can think of an equation in difference as a stochastic process where the noise term is always zero. So $\mathbb P (x_{t+1}=\rho x|x_t=x)=1$. Because $x_{t+1}$ is a function of $x_t$ but not of $(x_{t-1},x_{t-2}...)$ this difference equation satisfies the Markov property.}
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{10mm}
\section*{Problem 3}

\textbf{Credit:} Klaus Neusser \url{http://www2.econ.iastate.edu/classes/econ600/rksingh/fall16/TA/DifferenceEquations.pdf} on p. 19 (of the PDF)

\vspace{5mm}
\noindent
We study the dynamics of loan amortization. Denote by $D_t$ the amount of debt owed at time $t$. The debt contract is serviced by paying an amount $Z_t$ each period. $Z_t$ is given exogenously for this problem (you could imagine some agent optimizing in the background).

\begin{enumerate}[(a)]
\item Explain why debt dynamics are characterized by the equation
\begin{equation*}
	D_{t+1} = RD_t - Z_t
\end{equation*}
where $R$ is the constant gross interest rate associated with the loan. What kind of difference equation is this? Is this a forward or a backward equation? \\
\textcolor{blue}{The gross interest rate is $R=1+r$, so debt next period is debt at the current period plus the interest rate on debt minus repayment. It is a linear (time-homogeneous ??) first order difference equation. It is a forward equation, start with $D_0$.}

\item Suppose we start with an initial loan $D_0$. Solve iteratively (by induction) for $D_t$. You should get two terms --- explain the economics for both terms.\\
\textcolor{blue}{$$D_{t+1}=R^{t+1} D_0-\sum_{i=0}^t R^i Z_{t-i}$$. First term: Growth in debt if no repayment. Second term: NPV repayments (going back on time), as payment $Z$ at $t=0$ reduces debt at $t$ by $R^t$ compared to payment at $t$ }

\item Suppose the loan needs to be repaid at time $T$. Solve for the constant repayment schedule $Z_t = Z$ such that the loan is repaid in period $T$.\\
\textcolor{blue}{$$D_{t+1}=R^{t+1} D_0-\left(R^{t+1}-1\right) \frac{Z}{R-1}$$. If debt repaid at $T$, then must have $D_{T+1}=0$. Therefore $$Z=\frac{R-1}{1-R^{-T-1}}D_0$$}
\item What is the condition on constant repayment rate $Z$ relative to $D_0$ such that the loan is repaid in finite time? \\
\textcolor{blue}{Taking $T\rightarrow \infty$ we get $Z=(R-1)D_0$. So the payment is just the interest accruing in each period.}
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{10mm}
\section*{Problem 4}

\textbf{Credit:} Klaus Neusser \url{http://www2.econ.iastate.edu/classes/econ600/rksingh/fall16/TA/DifferenceEquations.pdf} on p. 38 (of the PDF)

\vspace{5mm}
\noindent
We study Cagan (1956)'s model for hyperinflation. The model is summarized by the three equations
\begin{align*}
	m_t^d - p_t &= \alpha (p_{t+1}^e - p_t) \\
	m_t^s &= m_t^d \\
	p_{t+1}^e - p_t &= \gamma(p_t - p_{t-1})
\end{align*}
where (all in logs) $m_t^d$ is money demand, $m_t^s$ is money supply, $p_t$ is the price level and $p_{t+1}^e$ is private agents' expectations for the price level in period $t+1$. The first equation of the above system characterizes money demand and the third equation characterizes \textit{adaptive} inflation expectations. Assume that $\alpha < 0$ and $\gamma > 0$.

\begin{enumerate}[(a)]
\item Characterize a first-order difference equation that solves for $p_t$ as a function of $p_{t-1}$ and $m_{t-1}$. What kind of difference equation is this? \\
\textcolor{blue}{Substitute the 3rd constraint into the 1st, $m_t^d-p_t=\alpha \gamma (p_t-p_{t-1})$. Let $m_t= m_t^d=m_t^s$. Then rearranging $$
p_t=\frac{\alpha \gamma}{1+\alpha \gamma} p_{t-1}+\frac{1}{1+\alpha \gamma} m_t \equiv \phi p_{t-1}+Z_t
$$ This is a linear (time-homogeneous ??) first order difference equation}
\item Using the tools already developed, solve for $p_t$ in terms of some initial conditions on the system.\\
\textcolor{blue}{Let $p_0$ be the initial price level, then iterating forward (as in the previous exercise) $$p_t=\phi^t p_0+\sum_{i=0}^{t-1} \phi^i Z_{t-i}$$}
\item Characterize the stability condition such that if $m_t \to m$ in the long run, $p_t$ converges to a steady state $p$. Interpret the economics of this stability condition. \\
\textcolor{blue}{To have both terms finite as $t\rightarrow \infty$ we need $$|\phi|=|\frac{\alpha \gamma}{1+\alpha \gamma}|<1$$ To have stability we need that money demand $m_t^d$ does not respond too much to current current inflation. This is the case if inflation expectations do not respond a lot to current inflation (small $\gamma$) and (or) the money demand is inelastic to inflation expectations (small $|\alpha|$). (REVISE)}

\item Explain why this equation should be thought of as a \textit{forward} equation.\\
\textcolor{blue}{Because in this model inflation expectations are adaptive, i.e. they are a function of past inflation. In the NK Phillips curve seen in class is the opposite because there we have rational expectations.}

\item Now assume that agents form expectations rationally instead of adaptively. That is, replace the third equation above by 
\begin{equation*}
	p_{t+1}^e = p_{t+1}.
\end{equation*}
Simplify the model equations again to obtain a difference equation for $p_t$ in terms of $m_t$. What kind of difference equation is this? \\
\textcolor{blue}{Substitute into the first equation and rearrange $$
p_{t+1}=\frac{\alpha-1}{\alpha} p_t+\frac{m_t}{\alpha}=\phi p_t+Z_t
$$ This is again a linear (time-homogeneous) first order difference equation. However now it is unstable because $\phi >1$}

\item Argue that we should think of this equation now as a \textit{backward} equation. Solve again for the \textit{general} solution of this difference equation (backwards), i.e., express $p_t$ in terms of $p_{t+s}$ and $m_{t+s}$.\\
\textcolor{blue}{With rational expectations, expected prices are a function of future prices. Therefore we need a terminal condition for the prices and this is a backward equation. Solving backwards from period $t+h$ up to $t$ $$
p_t = \phi^{-h} p_{t+h}-\phi^{-1} \sum_{i=0}^{h-1} \phi^{-i} Z_{t+i}
$$} 

\item Solve for a \textit{particular} solution by imposing some transversality (terminal) condition on $\lim_{T \to \infty} p_T$. 

\textcolor{blue}{Taking the limit in the equation above $$p_t=\lim_{h \to \infty}\phi^{-h}p_{t+h}-\phi^{-1} \sum_{i=0}^{\infty} \phi^{-i} Z_{t+i}$$
We impose the transversality condition $\lim_{T \to \infty} p_T=p$ and as $0<\phi^-1<1$ the limit remains finite.}

\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{10mm}
\section*{Problem 5}

Take the continuous-time limit of the following equations:
\begin{enumerate}[(a)]
\item $a_{t+1} = R_t a_t + y_t - c_t$ \\

\textcolor{blue}{Add and subtract $a_t$ and let $R_t = (1+r_t)a_t$. We have $$a_{t+\Delta}-a_t= \Delta (r_t a_t+y_t-c_t)$$
Divide by $\Delta$ and take the limit $$\dot a_t = r_t a_t + y_t - c_t$$ (Note its exactly the same steps as for the capital accumulation equation in the lectures).}

\item $u'(c_t) = \beta R_t u'(c_{t+1})$ \\

\textcolor{blue}{Let $R_t=1+r_t \Delta$ and $\beta = 1-\rho \Delta$, the Euler equation for time step $\Delta$ is $$u'(c_t)=(1+r_t \Delta )(1-\rho \Delta)u'(c_{t+\Delta})$$ The first order approximation of the last term around $c_t$ is $$u'(c_{t+\Delta})=u'(c_{t}+(c_{t+\Delta}-c_t))\approx u'(c_t)+u'(c_t)(c_{t+\Delta}-c_t)$$ Substitute back $$1=(1+r_t \Delta )(1-\rho \Delta)(1+\frac{u''(c_t)}{u'(c_t)}(c_{t+\Delta}-c_t))$$ Assume also CRRA utility so that $\gamma \equiv -\frac{u''(c_t)c_t}{u'(c_t)}$ is constant, then  $$1=(1+r_t \Delta )(1-\rho \Delta)(1-\gamma \frac{c_{t+\Delta}-c_t}{c_t})$$ $$(1+r_t \Delta-\rho \Delta - r_t \rho \Delta^2)\gamma \frac{c_{t+\Delta}-c_t}{c_t}=r_t \Delta-\rho \Delta - r_t \rho \Delta^2$$ Divide by $\Delta$ each side $$(1+r_t \Delta-\rho \Delta -r_t \rho \Delta^2)\gamma \frac{(c_{t+\Delta}-c_t)/\Delta}{c_t}=(r_t \Delta-\rho \Delta - r_t \rho \Delta^2)/\Delta$$ Taking $\Delta \rightarrow 0$ we get $$\frac{\dot c_t}{c_t} = \frac{r_t - \rho}{\gamma}$$}

\item $\pi_t = \beta \pi_{t+1} + \kappa x_t$ \\

\textcolor{blue}{As before, let $\beta = 1-\rho \Delta$ then the NKPC with time step $\Delta$ is $$\pi_{t+\Delta}-\pi_t = \rho \Delta \Pi_{t+\Delta}-\Delta \kappa x_t$$ Divide by $\Delta$ and take $\Delta \rightarrow 0$ $$\dot{\pi}_t = \rho \pi_t - \kappa x_t$$}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{10mm}
\section*{Problem 6}

Revert back from continuous to discrete time by plugging in for the definition of first-order derivative
\begin{equation*}
	\dot X_t = \frac{dX_t}{dt} \approx \frac{X_{t+\Delta} - X_t}{\Delta}
\end{equation*}
for small $\Delta$. 

\begin{enumerate}[(a)]
\item $\dot K_t = I_t - \delta K_t$

\textcolor{blue}{$\frac{K_{t+\Delta}-K_t}{\Delta}=I_t-\delta K_t$. Take $\Delta$ to 1 and rearrange.}


\item $\dot a_t = r_t a_t + y_t - c_t$

\textcolor{blue}{Same as a)}

\item $\frac{\dot C_t}{C_t} = \frac{r_t - \rho}{\gamma}$ \\

\textcolor{blue}{Basically we reverse the order of the steps in 5.b). We have $$ \frac{\dot C_t}{C_t} = \lim_{\Delta \rightarrow 0} (1+r_t \Delta-\rho \Delta - r_t \rho \Delta^2) \frac{(c_{t+\Delta}-c_t)/\Delta}{c_t} $$ $$ \frac{r_t - \rho}{\gamma}  =  \lim_{\Delta \rightarrow 0}  \frac{(r_t \Delta-\rho \Delta - r_t \rho \Delta^2)/\Delta}{\gamma} $$ Combining the two and substituting for gamma $$ (1+r_t \Delta-\rho \Delta - r_t \rho \Delta^2) (c_{t+\Delta}-c_t)u''(c_{t}) = - (r_t \Delta-\rho \Delta - r_t \rho \Delta^2) u'(c_t)$$ $$(1+r_t \Delta)(1-\rho \Delta) (c_{t+\Delta}-c_t)u''(c_{t})=(1-(1+r_t \Delta)(1-\rho \Delta))u'(c_t) $$ Using the first order approximation $$(1+r_t \Delta)(1-\rho \Delta) u'(c_{t+\Delta})=u'(c_t)$$ Taking $\Delta \rightarrow 1$ we get back the discrete time Euler equation}

\item $\dot \pi_t = \rho \pi_t - \kappa x_t$

\textcolor{blue}{We have $$\frac{\pi_{t+\Delta}-\pi_t}{\Delta}=\rho \pi_{t+1}-\kappa x_t$$ setting $\Delta = 1$ and rearranging $$\pi_t = (1-\rho)\pi_{t+1}+\kappa x_t = \beta \pi_{t+1}+\kappa x_t$$}
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{10mm}
\section*{Problem 7}

Consider the equation for wealth dynamics
\begin{equation*}
	\dot a_t = r_t a_t + y_t - c_t.
\end{equation*}
We take $\{r_t\}$ and $\{y_t - c_t\}$ as exogenously given.

\begin{enumerate}[(a)]
\item Solve for the lifetime budget constraint.

\item Solve the ODE for its general solution using the integrating factor method introduced in class, i.e., find an expression for $a_t$ in terms of the exogenous processes $\{r_t, y_t, c_t\}$ and some arbitrary initial condition $a_0 = c$. 

\textcolor{blue}{We do the same steps as in class only with the difference that $r$ (or $\delta$) is not constant: $$e^{-\int_0^t r_s ds}\dot{a}_t
-e^{-\int_0^t r_s ds}a_t = e^{-\int_0^t r_s ds}y_t - e^{-\int_0^t r_s ds} c_t$$ Notice the LHS is equal to $\frac{d}{dt}(a_t e^{-\int_0^t r_s ds})$. Integrate $$a_t e^{-\int_0^t r_s ds} = C+\int_0^t e^{-\int_0^t r_s ds}(y_s-c_s)ds$$ Setting $t=0$ we find $a_0=C$. For the lifetime budget, take $t \rightarrow \infty$ to get $$ a_0= \int_0^\infty e^{-\int_0^\infty r_s ds}(c_s-y_s)ds$$}


\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{10mm}
\section*{Problem 8}

\textbf{Credit:} Miranda Holmes-Cerfon \url{https://cims.nyu.edu/~holmes/teaching/asa19/handout_Lecture4_2019.pdf}

\vspace{5mm}
\noindent
In this problem, we will prove the Chapman-Kolmogorov Equation for a \textit{time-homogeneous} continuous-time Markov chains. Denote the \textit{transition probability} as
\begin{equation*}
	P_{ij}(t+s) = \mathbb P(X_{t+s} = j \mid X_t = i)
\end{equation*}
where $i$ and $j$ should be thought of as indices on the state space, i.e., the $i$th value of the finite state space $\mathcal X$ of the Markov chain.

Denote $I$ the indices associated with the state space $\mathcal X$. The Chapman-Kolmogorov equation is: 
\begin{equation*}
	P_{ij}(t+s) = \sum_{k \in I} P_{ik}(t) P_{kj}(s).
\end{equation*}

\begin{enumerate}[(a)]
\item Use the law of total probability to show that 
\begin{equation*}
	\mathbb P(X_{t+s} = j \mid X_0 = i) = \sum_k \mathbb P(X_{t+s} = j \mid X_t = k, X_0 = i) \mathbb P(X_t = k \mid X_0 = i) 
\end{equation*}

\textcolor{blue}{This is a direct application of the law of total probability, i.e. $P(A)=\sum_n P\left(A \mid B_n\right) P\left(B_n\right)$, applied with the probability mass function $\mathbb P( . \mid X_0 = i) $.}
\item Use the Markov property

\textcolor{blue}{By the Markov property $\mathbb P(X_{t+s} = j \mid X_t = k, X_0 = i)=\mathbb P(X_{t+s} = j \mid X_t = k)$}

\item Invoke time homogeneity to arrive at the result

\textcolor{blue}{The time-homogeneity implies that $\mathbb P(X_{t+s} = j \mid X_t = k)= P(X_{s} = j \mid X_0 = k)= P_{kj}(s)$}

\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{10mm}
\section*{Problem 9}

This problem collects several exercises on Brownian motion and stochastic calculus.

\begin{enumerate}[(a)]
\item Show that $\mathbb Cov(B_s, B_t) = \min\{s, t\}$ for two times $0 \leq s < t$. Use the following tricks: Use the covariance formula $\mathbb Cov(A, B) = \mathbb E (AB) - \mathbb E(A) \mathbb E(B)$. Use $B_t \sim \mathcal N(0, t)$ as well as $B_t - B_s \sim \mathcal N(0, t-s)$. And use $B_t = B_s + (B_t - B_s)$.

\textcolor{blue}{First we have  $\mathbb Cov(B_s, B_t) = \mathbb E (B_sB_t) - \mathbb E(B_s) \mathbb E(B_t) = \mathbb E (B_sB_t)  $. Then $$ \mathbb E (B_sB_t) =  \mathbb E (B_s(B_s+(B_t-B_s)))=\mathbb E (B_s^2)+ \mathbb E (B_s(B_t-B_s))$$ Notice that $B_s$ and $(B_t-B_s)$ are two independent random variables, hence $$\mathbb Cov(B_s, B_t) = s+\mathbb E (B_s) \mathbb E(B_t-B_s)=s$$}

\item Geometric Brownian motion evolves as: $dX_t = \mu X_t dt + \sigma X_t dB_t$. Show that
\begin{equation*}
	X_t = X_0 e^{\mu t - \frac{\sigma^2}{2} t + \sigma B_t}
\end{equation*}
for a given initial value $X_0$.

\textcolor{blue}{To derive the solution it will be useful to apply Ito's lemma to the function $f=log(X_t)$. Recall with a function of stochastic process we cannot use standard calculus, instead we can use Ito's lemma as a stochastic version of the chain rule. Applying Ito's lemma to $f$ $$df = dlog(X_t)= \partial_t f dt +\partial_X f \mu X_t dt+\frac{1}{2}\partial_{XX}f \sigma^2 X_t^2dt +\sigma X_t \partial_X f dB_t$$ The partial derivatives are: $\partial_t f = \frac{\partial log(X_t)}{\partial t}=0$, $\partial_X f = \frac{1}{X_t}$ and $\partial_{XX}f  = -\frac{1}{X_t^2}$. Substituting $$dlog(X_t) = (\mu - \frac{\sigma^2}{2})+\sigma dB_t$$ Integrate (and use $B_0=0$) $$log(X_t)-log(X_0)=(\mu - \frac{\sigma^2}{2})t+\sigma B_t$$ $$e^{log(\frac{X_t}{X_0})}=e^{\mu t - \frac{\sigma^2}{2} t + \sigma B_t}$$ Rearranging we get the result.}

\item For Geometric Brownian motion as defined above, show that $\mathbb E = X_0 e^{\mu t}$.

\textcolor{blue}{Taking expectations $$ \mathbb E (X_t)=  X_0 e^{\mu t-\frac{\sigma^2}{2}t}\mathbb E (e^{\sigma B_t})$$ Recall $ B_t \sim \mathcal N(0, t)$, it is useful to substitute $\sigma B_t = \sigma \sqrt t Z$ where  $Z \sim \mathcal N(0, 1)$. Then $$\mathbb E (e^{\sigma B_t}) = \int e^{\sigma \sqrt t Z}\frac{e^{-\frac{z^2}{2}}}{\sqrt{2\pi}}dz$$ For the term in the exponential we have $$\sigma \sqrt t Z-\frac{z^2}{2}=-\frac{1}{2}(z^2-2\sigma \sqrt{t}z) = -\frac{1}{2}(z -\sigma \sqrt{t})^2 +\frac{\sigma^2 t}{2} $$ Substitute back $$\mathbb E (e^{\sigma B_t}) = e^{\frac{\sigma^2 t}{2}}\int \frac{e^{-\frac{(z-\sigma \sqrt{t})^2}{2}}}{\sqrt{2\pi}}dz=e^{\frac{\sigma^2 t}{2}}$$ Because the second term is the density of a $\mathcal N (\sigma \sqrt t,1)$ which integrates to one. Substituting into the first equation we get the result.}

\item The Ornstein-Uhlenbeck (OU) process is like a continuous-time variant of the AR(1) process. It evolves as $dX_t = - \mu X_t dt + \sigma dB_t$ for drift parameter $\mu$, diffusion parameter $\sigma$, and some $X$. Show that it solves 
\begin{equation*}
	X_t = X_0 e^{- \mu t} + \sigma \int_0^t e^{-\mu(t - s)} dB_s.
\end{equation*}
\end{enumerate}

\textcolor{blue}{As before we derive the solution by applying the Ito's lemma to a properly chosen function. Let $f=e^{\mu t}X_t$, applying Ito's lemma
$$df = de^{\mu t}X_t = \partial_t f dt + \partial_X f (-\mu X_t)dt+\frac{1}{2}\partial_{XX} f \sigma ^2 dt + \sigma \partial_X f dB_t$$ Substitute the partial derivatives $$de^{\mu t}X_t = \mu e^{\mu t}X_t dt + -\mu X_t e^{\mu t} dt+ 0 + \sigma e^{\mu t} dB_t$$ Integrate: $$e^{\mu t}X_t-X_0= \sigma \int_0^te^{\mu s}dB_s$$ Multiplying by $e^{-\mu t}$ we get the result.}





\end{document}











